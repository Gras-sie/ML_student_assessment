{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7fc999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, accuracy_score, ConfusionMatrixDisplay\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "#Load your data\n",
    "train_data = pd.read_csv(\"../Data/train_data.csv\")\n",
    "test_data = pd.read_csv(\"../Data/test_data.csv\")\n",
    "\n",
    "#Separate features and labels\n",
    "X_train = train_data.drop(columns=[\"GradeClass\", \"StudentID\"])\n",
    "y_train = train_data[\"GradeClass\"].astype(int)\n",
    "\n",
    "X_test = test_data.drop(columns=[\"GradeClass\", \"StudentID\"])\n",
    "y_test = test_data[\"GradeClass\"].astype(int)\n",
    "\n",
    "#Train XGBoost Model (no scaling needed)\n",
    "model = XGBClassifier(n_estimators=100, eval_metric='mlogloss', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Grade mapping\n",
    "grade_map = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'F'}\n",
    "y_test_letters = pd.Series(y_test).map(grade_map)\n",
    "y_pred_letters = pd.Series(y_pred).map(grade_map)\n",
    "label_names = [grade_map[i] for i in sorted(grade_map)]\n",
    "\n",
    "#Evaluation\n",
    "accuracy = accuracy_score(y_test_letters, y_pred_letters)\n",
    "print(\"XGBoost Accuracy:\", round(accuracy, 4), \"\\n────────────────────────\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_letters, y_pred_letters, target_names=label_names, zero_division=0))\n",
    "\n",
    "#Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    display_labels=label_names,\n",
    "    cmap='Blues'\n",
    ")\n",
    "plt.title(f\"Confusion Matrix (Accuracy: {round(accuracy, 4)})\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Save model only (no scaler needed)\n",
    "output_model_path = \"../Artifacts/PLK\"\n",
    "os.makedirs(output_model_path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_model_path, \"xgboost_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "#Prediction comparison table\n",
    "comparison_df = test_data[[\"StudentID\", \"GradeClass\"]].copy()\n",
    "comparison_df[\"Predicted\"] = y_pred\n",
    "comparison_df[\"ActualGrade\"] = comparison_df[\"GradeClass\"].astype(int).map(grade_map)\n",
    "comparison_df[\"PredictedGrade\"] = pd.Series(y_pred).astype(int).map(grade_map)\n",
    "comparison_df[\"Match\"] = comparison_df[\"ActualGrade\"] == comparison_df[\"PredictedGrade\"]\n",
    "comparison_df = comparison_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#Save prediction table\n",
    "os.makedirs(\"project testing\", exist_ok=True)\n",
    "comparison_df.to_csv(\"../Artifacts/Predictions/prediction_results_xgboost.csv\", index=False)\n",
    "\n",
    "#Display sample\n",
    "try:\n",
    "    from IPython.display import display\n",
    "\n",
    "    def highlight_false_text(row):\n",
    "        styles = []\n",
    "        for col in row.index:\n",
    "            if col == \"Match\" and row[\"Match\"] == False:\n",
    "                styles.append(\"color: red; background-color: black\")\n",
    "            else:\n",
    "                styles.append(\"background-color: black; color: white\")\n",
    "        return styles\n",
    "\n",
    "    print(\"First 20 Predictions:\")\n",
    "    display(comparison_df.head(20).style.apply(highlight_false_text, axis=1))\n",
    "except:\n",
    "    print(\"\\nFirst 20 Predictions:\")\n",
    "    print\n",
    "\n",
    "# Grade mapping\n",
    "grade_map = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'F'}\n",
    "y_pred_letters = pd.Series(y_pred).map(grade_map)\n",
    "\n",
    "# Count predicted grades\n",
    "predicted_counts = y_pred_letters.value_counts().reindex(['A', 'B', 'C', 'D', 'F'], fill_value=0)\n",
    "\n",
    "# Create summary DataFrame\n",
    "grade_summary_df = pd.DataFrame({\n",
    "    \"Grade\": predicted_counts.index,\n",
    "    \"Count\": predicted_counts.values\n",
    "})\n",
    "\n",
    "# Add total row\n",
    "total_row = pd.DataFrame([{\"Grade\": \"Total\", \"Count\": grade_summary_df[\"Count\"].sum()}])\n",
    "grade_summary_df = pd.concat([grade_summary_df, total_row], ignore_index=True)\n",
    "\n",
    "# Display table\n",
    "print(grade_summary_df.to_string(index=False))\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(grade_summary_df[\"Grade\"], grade_summary_df[\"Count\"], color='skyblue')\n",
    "for i, count in enumerate(grade_summary_df[\"Count\"]):\n",
    "    plt.text(i, count + 2, str(count), ha='center', va='bottom', fontsize=10)\n",
    "plt.title(\"Predicted Grade Distribution\")\n",
    "plt.xlabel(\"Grade\")\n",
    "plt.ylabel(\"Number of Students\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
